{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ebaf1d8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7428c2-a093-43a0-b349-b7e3be446c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Python Path: ['/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python311.zip', '/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11', '/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/lib-dynload', '', '/Users/pamelasanchezhernandez/AfroVision/venv/lib/python3.11/site-packages', '/Users/pamelasanchezhernandez/AfroVision/MODNet/src']\n",
      "âœ… Pretrained models found! Loading MODNet...\n",
      "cannot find the pretrained mobilenetv2 backbone\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Pretrained models found! Loading MODNet...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# âœ… Load MODNet Model\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m modnet \u001b[38;5;241m=\u001b[39m \u001b[43mMODNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackbone_pretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# âœ… Force MODNet to load backbone\u001b[39;00m\n\u001b[1;32m     33\u001b[0m modnet\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(modnet_ckpt, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)), strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     34\u001b[0m modnet\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/AfroVision/MODNet/src/models/modnet.py:229\u001b[0m, in \u001b[0;36mMODNet.__init__\u001b[0;34m(self, in_channels, hr_channels, backbone_arch, backbone_pretrained)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_norm(m)\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone_pretrained:\n\u001b[0;32m--> 229\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_pretrained_ckpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AfroVision/MODNet/src/models/backbones/wrapper.py:91\u001b[0m, in \u001b[0;36mMobileNetV2Backbone.load_pretrained_ckpt\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(ckpt_path):\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcannot find the pretrained mobilenetv2 backbone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 91\u001b[0m     \u001b[43mexit\u001b[49m()\n\u001b[1;32m     93\u001b[0m ckpt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(ckpt_path)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mload_state_dict(ckpt)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# âœ… Ensure MODNet/src is in the Python path\n",
    "modnet_path = \"/Users/pamelasanchezhernandez/AfroVision/MODNet/src\"\n",
    "if modnet_path not in sys.path:\n",
    "    sys.path.append(modnet_path)\n",
    "\n",
    "# âœ… Set paths for pre-trained models\n",
    "pretrained_path = \"/Users/pamelasanchezhernandez/AfroVision/MODNet/pretrained/\"\n",
    "modnet_ckpt = os.path.join(pretrained_path, \"modnet_photographic_portrait_matting.ckpt\")\n",
    "mobilenetv2_ckpt = os.path.join(pretrained_path, \"mobilenetv2_backbone.ckpt\")\n",
    "\n",
    "# âœ… Check if models exist\n",
    "if not os.path.exists(modnet_ckpt):\n",
    "    raise FileNotFoundError(f\"âŒ MODNet checkpoint not found at {modnet_ckpt}\")\n",
    "if not os.path.exists(mobilenetv2_ckpt):\n",
    "    raise FileNotFoundError(f\"âŒ MobileNetV2 backbone not found at {mobilenetv2_ckpt}\")\n",
    "\n",
    "print(\"âœ… Pretrained models found! Loading MODNet...\")\n",
    "\n",
    "# âœ… Tell PyTorch where to find the models\n",
    "os.environ[\"TORCH_HOME\"] = pretrained_path\n",
    "\n",
    "# âœ… Import MODNet\n",
    "from models.modnet import MODNet\n",
    "\n",
    "# âœ… Load MODNet Model with Correct Backbone\n",
    "modnet = MODNet(backbone_pretrained=False)  # ðŸ”¹ Change to False to manually load\n",
    "modnet.backbone.load_pretrained_ckpt = lambda: torch.load(mobilenetv2_ckpt, map_location=torch.device(\"cpu\"))\n",
    "\n",
    "modnet.load_state_dict(torch.load(modnet_ckpt, map_location=torch.device(\"cpu\")), strict=False)\n",
    "modnet.eval()  # Set model to evaluation mode\n",
    "\n",
    "print(\"âœ… MODNet loaded successfully!\")\n",
    "\n",
    "# âœ… Load and Preprocess Image\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"Loads and preprocesses an image for MODNet.\"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = image.resize((512, 512))  # Resize to model input size\n",
    "    image = np.array(image).astype(np.float32) / 255.0  # Normalize\n",
    "    image = torch.tensor(image).permute(2, 0, 1).unsqueeze(0)  # Convert to tensor\n",
    "    return image\n",
    "\n",
    "# âœ… Update the image path if necessary\n",
    "image_path = \"/Users/pamelasanchezhernandez/AfroVision/sample.jpg\"\n",
    "input_image = preprocess_image(image_path)\n",
    "\n",
    "# âœ… Apply MODNet Segmentation (Fix: Pass inference=True)\n",
    "with torch.no_grad():\n",
    "    matte = modnet(input_image, inference=True)[0]  # MODNet returns a list, so take the first item\n",
    "    mask = matte[0].cpu().numpy()  # Extract mask\n",
    "\n",
    "# âœ… Convert the Mask to a Binary Format\n",
    "mask = (mask > 0.5).astype(np.uint8) * 255  # Thresholding\n",
    "\n",
    "# âœ… Show Original and Masked Images\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(Image.open(image_path))\n",
    "ax[0].set_title(\"Original Image\")\n",
    "ax[1].imshow(mask, cmap=\"gray\")\n",
    "ax[1].set_title(\"Curly Hair Segmentation Mask\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417dbfa1-3ae2-4ee5-874e-41f79b4dc9af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
